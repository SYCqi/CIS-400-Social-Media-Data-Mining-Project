{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYMQ1PFiBJeA",
        "outputId": "f5207dc9-e8b2-4dc1-a97e-b7aab0c032e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "fatal: destination path 'CIS-400-Social-Media-Data-Mining-Project' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/CIS-400-Social-Media-Data-Mining-Project\n",
            "Your branch is up to date with 'origin/main'.\n",
            "/content/drive/MyDrive/CIS-400-Social-Media-Data-Mining-Project/ML and roBERTa Sentiment Analysis\n"
          ]
        }
      ],
      "source": [
        "# we need to save Reddit data to your Google Drive, so connect that first\n",
        "# the data is preprocessed weather related posts/comments from 2 state\n",
        "# subreddits for each of Cold, Temperate, and Hot regions\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/\n",
        "!git clone --no-checkout https://github.com/SYCqi/CIS-400-Social-Media-Data-Mining-Project.git\n",
        "%cd /content/drive/MyDrive/CIS-400-Social-Media-Data-Mining-Project\n",
        "!git sparse-checkout init --cone\n",
        "!git sparse-checkout set ML and roBERTa Sentiment Analysis/preprocessedData\n",
        "!git checkout\n",
        "%cd ML_Manu/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsF5dK14Fxfs",
        "outputId": "f1d16f32-5582-407d-afce-63a7606d3e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file: cold_cloudy.db\n"
          ]
        }
      ],
      "source": [
        "# this script goes through the SQLite .db files in preprocessedData directory\n",
        "# truncates for roBERTa model and performs sentiment analysis\n",
        "\n",
        "import os\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model_name)\n",
        "\n",
        "def truncate_and_analyze_sentiment(text, max_length=512):\n",
        "  # roberta does pos/neg/neutral classification but has max token length of 512\n",
        "  # ideally use sliding window to process but for expediency's sake I'll trunc\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=max_length)\n",
        "    truncated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
        "    # Perform sentiment analysis\n",
        "    result = sentiment_analyzer(truncated_text)\n",
        "    # Map result to positive, neutral, negative\n",
        "    sentiment = result[0]\n",
        "    label_mapping = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n",
        "\n",
        "    sentiment_scores = {\n",
        "        label_mapping.get(sentiment[\"label\"], \"unknown\"): sentiment[\"score\"]\n",
        "    }\n",
        "\n",
        "    return sentiment_scores\n",
        "\n",
        "def process_database(file_path):\n",
        "    conn = sqlite3.connect(file_path)\n",
        "    query = \"SELECT TEXT FROM RedditPosts\"  # our data is in RedditPosts\n",
        "    data = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "\n",
        "    # preprocessedData should already be good\n",
        "    data = data[data['TEXT'].notnull()]  # Remove nulls\n",
        "    data = data[data['TEXT'].str.strip() != \"\"]  # Remove empty rows\n",
        "\n",
        "    # Analyze sentiment w/ Hugging Face model\n",
        "    sentiments = []\n",
        "    for text in data['TEXT']:\n",
        "        sentiment = truncate_and_analyze_sentiment(text)\n",
        "        sentiments.append(sentiment)\n",
        "\n",
        "    # Add sentiment scores to the DataFrame\n",
        "    data['Positive'] = [s.get('positive', 0) for s in sentiments]\n",
        "    data['Neutral'] = [s.get('neutral', 0) for s in sentiments]\n",
        "    data['Negative'] = [s.get('negative', 0) for s in sentiments]\n",
        "\n",
        "    # Return the data with sentiment scores\n",
        "    return data\n",
        "\n",
        "def process_directory(directory_path):\n",
        "    all_data = pd.DataFrame()\n",
        "\n",
        "    # Iterate through all files in the directory\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith(\".db\"):  # Only process .db files\n",
        "            db_path = os.path.join(directory_path, file_name)\n",
        "            print(f\"Processing file: {file_name}\")\n",
        "\n",
        "            # Process the database and get sentiment results\n",
        "            region_data = process_database(db_path)\n",
        "\n",
        "            # region and weather type)\n",
        "            region, weather = file_name.replace(\".db\", \"\").split(\"_\")\n",
        "            region_data['Region'] = region\n",
        "            region_data['Weather'] = weather\n",
        "\n",
        "            # Append the region data to the all_data DataFrame\n",
        "            all_data = pd.concat([all_data, region_data], ignore_index=True)\n",
        "\n",
        "            # Plot results for the individual region\n",
        "            plot_sentiments(region_data, region)\n",
        "\n",
        "    # Plot the aggregated results for all regions\n",
        "    plot_sentiments(all_data, \"All Regions\")\n",
        "\n",
        "def plot_sentiments(data, region_name):\n",
        "    # Aggregate scores by sentiment\n",
        "    total_sentiments = data[['Positive', 'Neutral', 'Negative']].mean()\n",
        "\n",
        "    cap_name = region_name.capitalize()\n",
        "    # Plot a bar chart\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    total_sentiments.plot(kind='bar', color=['green', 'grey', 'red'], alpha=0.7)\n",
        "    plt.title(f\"Average Sentiment Strength Scores for {cap_name} Region\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.xlabel(\"Sentiment Type\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.savefig(f\"{region_name}_sentiment_bar_chart.png\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    total_sentiments.plot(kind='pie', autopct='%1.1f%%', colors=['green', 'grey', 'red'], startangle=90)\n",
        "    plt.title(f\"Sentiment Distribution for {cap_name} Region\")\n",
        "    plt.ylabel(\"\")  # Hide y-axis label\n",
        "    plt.savefig(f\"{region_name}_sentiment_pie_chart.png\")\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    directory_path = \"preprocessedData\"  # replace if using a different dir\n",
        "    process_directory(directory_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
